{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPLSZ5VYm5g4IsHKyUwnq6W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RandyTunru/FashionMNIST_Classification/blob/main/Fashion_MNIST_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ayPCipOoUiMq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import datasets\n",
        "from torchvision.models import resnet\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn as nn\n",
        "from torchvision.transforms import ToTensor"
      ],
      "metadata": {
        "id": "PNvN3UXiUqq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(2510)\n",
        "torch.manual_seed(2510)"
      ],
      "metadata": {
        "id": "udibYyEMq7E1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ceedb97-c5aa-4f69-bd7d-2c3d27ea67bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7820cdfa3790>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "# Download test data from open datasets.\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")"
      ],
      "metadata": {
        "id": "5ohQp5OyU87T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cb62bd5-ab21-46b9-dee9-37f7fa019181"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:02<00:00, 12.0MB/s]\n",
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 203kB/s]\n",
            "100%|██████████| 4.42M/4.42M [00:01<00:00, 3.76MB/s]\n",
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 13.0MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(training_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pog0uR8DcLOJ",
        "outputId": "1eb31f26-9b34-4f8b-d98e-fe952426da19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset FashionMNIST\n",
            "    Number of datapoints: 60000\n",
            "    Root location: data\n",
            "    Split: Train\n",
            "    StandardTransform\n",
            "Transform: ToTensor()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "# Create data loaders.\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "for X, y in test_dataloader:\n",
        "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
        "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_5T-afzVfRc",
        "outputId": "0e9954aa-f632-47d3-b397-7c067f2667d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(train_dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        },
        "id": "Zam1zMlwVkHB",
        "outputId": "678afb03-36b8-4727-af01-7d464a7ffc92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.utils.data.dataloader.DataLoader"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>torch.utils.data.dataloader.DataLoader</b><br/>def __init__(dataset: Dataset[_T_co], batch_size: Optional[int]=1, shuffle: Optional[bool]=None, sampler: Union[Sampler, Iterable, None]=None, batch_sampler: Union[Sampler[list], Iterable[list], None]=None, num_workers: int=0, collate_fn: Optional[_collate_fn_t]=None, pin_memory: bool=False, drop_last: bool=False, timeout: float=0, worker_init_fn: Optional[_worker_init_fn_t]=None, multiprocessing_context=None, generator=None, *, prefetch_factor: Optional[int]=None, persistent_workers: bool=False, pin_memory_device: str=&#x27;&#x27;, in_order: bool=True) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py</a>Data loader combines a dataset and a sampler, and provides an iterable over the given dataset.\n",
              "\n",
              "The :class:`~torch.utils.data.DataLoader` supports both map-style and\n",
              "iterable-style datasets with single- or multi-process loading, customizing\n",
              "loading order and optional automatic batching (collation) and memory pinning.\n",
              "\n",
              "See :py:mod:`torch.utils.data` documentation page for more details.\n",
              "\n",
              "Args:\n",
              "    dataset (Dataset): dataset from which to load the data.\n",
              "    batch_size (int, optional): how many samples per batch to load\n",
              "        (default: ``1``).\n",
              "    shuffle (bool, optional): set to ``True`` to have the data reshuffled\n",
              "        at every epoch (default: ``False``).\n",
              "    sampler (Sampler or Iterable, optional): defines the strategy to draw\n",
              "        samples from the dataset. Can be any ``Iterable`` with ``__len__``\n",
              "        implemented. If specified, :attr:`shuffle` must not be specified.\n",
              "    batch_sampler (Sampler or Iterable, optional): like :attr:`sampler`, but\n",
              "        returns a batch of indices at a time. Mutually exclusive with\n",
              "        :attr:`batch_size`, :attr:`shuffle`, :attr:`sampler`,\n",
              "        and :attr:`drop_last`.\n",
              "    num_workers (int, optional): how many subprocesses to use for data\n",
              "        loading. ``0`` means that the data will be loaded in the main process.\n",
              "        (default: ``0``)\n",
              "    collate_fn (Callable, optional): merges a list of samples to form a\n",
              "        mini-batch of Tensor(s).  Used when using batched loading from a\n",
              "        map-style dataset.\n",
              "    pin_memory (bool, optional): If ``True``, the data loader will copy Tensors\n",
              "        into device/CUDA pinned memory before returning them.  If your data elements\n",
              "        are a custom type, or your :attr:`collate_fn` returns a batch that is a custom type,\n",
              "        see the example below.\n",
              "    drop_last (bool, optional): set to ``True`` to drop the last incomplete batch,\n",
              "        if the dataset size is not divisible by the batch size. If ``False`` and\n",
              "        the size of dataset is not divisible by the batch size, then the last batch\n",
              "        will be smaller. (default: ``False``)\n",
              "    timeout (numeric, optional): if positive, the timeout value for collecting a batch\n",
              "        from workers. Should always be non-negative. (default: ``0``)\n",
              "    worker_init_fn (Callable, optional): If not ``None``, this will be called on each\n",
              "        worker subprocess with the worker id (an int in ``[0, num_workers - 1]``) as\n",
              "        input, after seeding and before data loading. (default: ``None``)\n",
              "    multiprocessing_context (str or multiprocessing.context.BaseContext, optional): If\n",
              "        ``None``, the default\n",
              "        `multiprocessing context &lt;https://docs.python.org/3/library/multiprocessing.html#contexts-and-start-methods&gt;`_ # noqa: D401\n",
              "        of your operating system will\n",
              "        be used. (default: ``None``)\n",
              "    generator (torch.Generator, optional): If not ``None``, this RNG will be used\n",
              "        by RandomSampler to generate random indexes and multiprocessing to generate\n",
              "        ``base_seed`` for workers. (default: ``None``)\n",
              "    prefetch_factor (int, optional, keyword-only arg): Number of batches loaded\n",
              "        in advance by each worker. ``2`` means there will be a total of\n",
              "        2 * num_workers batches prefetched across all workers. (default value depends\n",
              "        on the set value for num_workers. If value of num_workers=0 default is ``None``.\n",
              "        Otherwise, if value of ``num_workers &gt; 0`` default is ``2``).\n",
              "    persistent_workers (bool, optional): If ``True``, the data loader will not shut down\n",
              "        the worker processes after a dataset has been consumed once. This allows to\n",
              "        maintain the workers `Dataset` instances alive. (default: ``False``)\n",
              "    pin_memory_device (str, optional): the device to :attr:`pin_memory` on if ``pin_memory`` is\n",
              "        ``True``. If not given, the current :ref:`accelerator&lt;accelerators&gt;` will be the\n",
              "        default. This argument is discouraged and subject to deprecated.\n",
              "    in_order (bool, optional): If ``False``, the data loader will not enforce that batches\n",
              "        are returned in a first-in, first-out order. Only applies when ``num_workers &gt; 0``. (default: ``True``)\n",
              "\n",
              "\n",
              ".. warning:: If the ``spawn`` start method is used, :attr:`worker_init_fn`\n",
              "             cannot be an unpicklable object, e.g., a lambda function. See\n",
              "             :ref:`multiprocessing-best-practices` on more details related\n",
              "             to multiprocessing in PyTorch.\n",
              "\n",
              ".. warning:: ``len(dataloader)`` heuristic is based on the length of the sampler used.\n",
              "             When :attr:`dataset` is an :class:`~torch.utils.data.IterableDataset`,\n",
              "             it instead returns an estimate based on ``len(dataset) / batch_size``, with proper\n",
              "             rounding depending on :attr:`drop_last`, regardless of multi-process loading\n",
              "             configurations. This represents the best guess PyTorch can make because PyTorch\n",
              "             trusts user :attr:`dataset` code in correctly handling multi-process\n",
              "             loading to avoid duplicate data.\n",
              "\n",
              "             However, if sharding results in multiple workers having incomplete last batches,\n",
              "             this estimate can still be inaccurate, because (1) an otherwise complete batch can\n",
              "             be broken into multiple ones and (2) more than one batch worth of samples can be\n",
              "             dropped when :attr:`drop_last` is set. Unfortunately, PyTorch can not detect such\n",
              "             cases in general.\n",
              "\n",
              "             See `Dataset Types`_ for more details on these two types of datasets and how\n",
              "             :class:`~torch.utils.data.IterableDataset` interacts with\n",
              "             `Multi-process data loading`_.\n",
              "\n",
              ".. warning:: See :ref:`reproducibility`, and :ref:`dataloader-workers-random-seed`, and\n",
              "             :ref:`data-loading-randomness` notes for random seed related questions.\n",
              "\n",
              ".. warning:: Setting `in_order` to `False` can harm reproducibility and may lead to a skewed data\n",
              "             distribution being fed to the trainer in cases with imbalanced data.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 135);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "# Define model\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "fc_model = NeuralNetwork().to(device)\n",
        "print(fc_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhbPTDQejzBc",
        "outputId": "425e543e-9ade-4984-9c8c-2e24c765794e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n",
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvolutionModel(nn.Module):\n",
        "    def __init__(self, n_channels, n_class):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.conv_stack1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=n_channels, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "\n",
        "        conv_output_size = (28 - 3 + 2 * 1) // 1 + 1\n",
        "        max_pool_output_size = (conv_output_size - 2) // 2 + 1\n",
        "\n",
        "        self.conv_stack2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "\n",
        "        conv_output_size = (max_pool_output_size - 3 + 2 * 1) // 1 + 1\n",
        "        max_pool_output_size = (conv_output_size - 2) // 2 + 1\n",
        "\n",
        "\n",
        "        with torch.no_grad():\n",
        "          dummy = torch.ones(1, n_channels, 28, 28)\n",
        "          dummy = self.conv_stack1(dummy)\n",
        "          dummy = self.conv_stack2(dummy)\n",
        "          dummy = self.flatten(dummy)\n",
        "          flatten_dim = dummy.view(-1).shape[0]\n",
        "\n",
        "\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(flatten_dim, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "        self.classifier = nn.Linear(512, n_class)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_stack1(x)\n",
        "        x = self.conv_stack2(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.linear_relu_stack(x)\n",
        "        logits = self.classifier(x)\n",
        "        return logits\n",
        "\n",
        "custom_cnn_model = ConvolutionModel(n_channels=1, n_class=10).to(device)\n",
        "print(custom_cnn_model)"
      ],
      "metadata": {
        "id": "CCmmZKKRoUac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e0836e8-11bd-42cd-b0f7-1375da71272e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ConvolutionModel(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (conv_stack1): Sequential(\n",
            "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv_stack2): Sequential(\n",
            "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=3136, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): ReLU()\n",
            "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (4): ReLU()\n",
            "  )\n",
            "  (classifier): Linear(in_features=512, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resnet_18_model = resnet.resnet18(num_classes=10).to(device)\n",
        "\n",
        "resnet_18_model.conv1 = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
        "resnet_18_model.maxpool = nn.Identity()\n",
        "\n",
        "nn.init.kaiming_normal_(resnet_18_model.conv1.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "\n",
        "resnet_18_model = resnet_18_model.to(device)\n",
        "\n",
        "print(resnet_18_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26xQ5Rrf8rvO",
        "outputId": "36fd2c8a-9967-4cc9-948a-e8ebd4905178"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): Identity()\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "o_3OyziRkKHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataloader, model, loss_fn):\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
        "\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Compute prediction error\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), (batch + 1) * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
      ],
      "metadata": {
        "id": "GDfM2L7BkPa6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ],
      "metadata": {
        "id": "fnIOqHrBkS8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(dataloader, model):\n",
        "    model.eval()\n",
        "    pred = None\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "\n",
        "    return pred"
      ],
      "metadata": {
        "id": "BZIzXAackjLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 5\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, fc_model, loss_fn)\n",
        "    test(test_dataloader, fc_model, loss_fn)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gL6Vti0ckTw7",
        "outputId": "04c96afc-992e-444c-8ddd-dd8909f91835"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.304441  [   64/60000]\n",
            "loss: 2.297501  [ 6464/60000]\n",
            "loss: 2.268801  [12864/60000]\n",
            "loss: 2.264300  [19264/60000]\n",
            "loss: 2.242896  [25664/60000]\n",
            "loss: 2.209267  [32064/60000]\n",
            "loss: 2.224827  [38464/60000]\n",
            "loss: 2.183050  [44864/60000]\n",
            "loss: 2.179285  [51264/60000]\n",
            "loss: 2.150638  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 42.6%, Avg loss: 2.140314 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.146119  [   64/60000]\n",
            "loss: 2.144505  [ 6464/60000]\n",
            "loss: 2.073084  [12864/60000]\n",
            "loss: 2.100970  [19264/60000]\n",
            "loss: 2.028572  [25664/60000]\n",
            "loss: 1.971592  [32064/60000]\n",
            "loss: 2.010378  [38464/60000]\n",
            "loss: 1.917474  [44864/60000]\n",
            "loss: 1.924077  [51264/60000]\n",
            "loss: 1.863693  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 57.9%, Avg loss: 1.851349 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.875571  [   64/60000]\n",
            "loss: 1.859306  [ 6464/60000]\n",
            "loss: 1.725130  [12864/60000]\n",
            "loss: 1.782429  [19264/60000]\n",
            "loss: 1.646529  [25664/60000]\n",
            "loss: 1.613898  [32064/60000]\n",
            "loss: 1.640430  [38464/60000]\n",
            "loss: 1.537557  [44864/60000]\n",
            "loss: 1.564120  [51264/60000]\n",
            "loss: 1.469928  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 60.9%, Avg loss: 1.481943 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.544779  [   64/60000]\n",
            "loss: 1.523611  [ 6464/60000]\n",
            "loss: 1.360284  [12864/60000]\n",
            "loss: 1.441977  [19264/60000]\n",
            "loss: 1.309669  [25664/60000]\n",
            "loss: 1.319368  [32064/60000]\n",
            "loss: 1.331638  [38464/60000]\n",
            "loss: 1.259635  [44864/60000]\n",
            "loss: 1.296667  [51264/60000]\n",
            "loss: 1.202421  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 63.0%, Avg loss: 1.228433 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.304186  [   64/60000]\n",
            "loss: 1.297814  [ 6464/60000]\n",
            "loss: 1.119799  [12864/60000]\n",
            "loss: 1.230495  [19264/60000]\n",
            "loss: 1.100383  [25664/60000]\n",
            "loss: 1.131546  [32064/60000]\n",
            "loss: 1.150455  [38464/60000]\n",
            "loss: 1.092204  [44864/60000]\n",
            "loss: 1.132003  [51264/60000]\n",
            "loss: 1.054109  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 64.6%, Avg loss: 1.074962 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 5\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, custom_cnn_model, loss_fn)\n",
        "    test(test_dataloader, custom_cnn_model, loss_fn)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "keCyn3FI-EE-",
        "outputId": "e29f801e-c5b8-4314-b1ec-c51c0bacfdac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.303164  [   64/60000]\n",
            "loss: 2.301698  [ 6464/60000]\n",
            "loss: 2.300458  [12864/60000]\n",
            "loss: 2.296741  [19264/60000]\n",
            "loss: 2.299154  [25664/60000]\n",
            "loss: 2.293104  [32064/60000]\n",
            "loss: 2.288279  [38464/60000]\n",
            "loss: 2.286369  [44864/60000]\n",
            "loss: 2.289739  [51264/60000]\n",
            "loss: 2.280201  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 25.8%, Avg loss: 2.284006 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.285421  [   64/60000]\n",
            "loss: 2.285273  [ 6464/60000]\n",
            "loss: 2.278732  [12864/60000]\n",
            "loss: 2.277332  [19264/60000]\n",
            "loss: 2.275336  [25664/60000]\n",
            "loss: 2.266762  [32064/60000]\n",
            "loss: 2.263536  [38464/60000]\n",
            "loss: 2.256625  [44864/60000]\n",
            "loss: 2.260794  [51264/60000]\n",
            "loss: 2.243544  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 37.5%, Avg loss: 2.246774 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 2.252177  [   64/60000]\n",
            "loss: 2.249072  [ 6464/60000]\n",
            "loss: 2.229937  [12864/60000]\n",
            "loss: 2.228146  [19264/60000]\n",
            "loss: 2.207518  [25664/60000]\n",
            "loss: 2.191470  [32064/60000]\n",
            "loss: 2.179747  [38464/60000]\n",
            "loss: 2.151339  [44864/60000]\n",
            "loss: 2.144059  [51264/60000]\n",
            "loss: 2.086901  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 40.3%, Avg loss: 2.085814 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 2.105592  [   64/60000]\n",
            "loss: 2.068500  [ 6464/60000]\n",
            "loss: 1.977966  [12864/60000]\n",
            "loss: 1.941391  [19264/60000]\n",
            "loss: 1.802343  [25664/60000]\n",
            "loss: 1.744369  [32064/60000]\n",
            "loss: 1.652726  [38464/60000]\n",
            "loss: 1.543586  [44864/60000]\n",
            "loss: 1.499049  [51264/60000]\n",
            "loss: 1.371371  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 57.8%, Avg loss: 1.366189 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.441030  [   64/60000]\n",
            "loss: 1.363236  [ 6464/60000]\n",
            "loss: 1.199805  [12864/60000]\n",
            "loss: 1.230145  [19264/60000]\n",
            "loss: 1.074079  [25664/60000]\n",
            "loss: 1.129704  [32064/60000]\n",
            "loss: 1.107341  [38464/60000]\n",
            "loss: 1.025275  [44864/60000]\n",
            "loss: 1.062943  [51264/60000]\n",
            "loss: 1.027379  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 62.2%, Avg loss: 0.999035 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, resnet_18_model, loss_fn)\n",
        "    test(test_dataloader, resnet_18_model, loss_fn)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfqxCfue-Mh4",
        "outputId": "a6885a67-feae-4886-d8fd-68546c726404"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.394788  [   64/60000]\n",
            "loss: 2.122333  [ 6464/60000]\n",
            "loss: 1.937794  [12864/60000]\n",
            "loss: 1.833347  [19264/60000]\n",
            "loss: 1.581503  [25664/60000]\n",
            "loss: 1.498508  [32064/60000]\n",
            "loss: 1.391025  [38464/60000]\n",
            "loss: 1.246103  [44864/60000]\n",
            "loss: 1.177441  [51264/60000]\n",
            "loss: 1.095626  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 71.3%, Avg loss: 1.065360 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.074306  [   64/60000]\n",
            "loss: 1.047838  [ 6464/60000]\n",
            "loss: 0.783777  [12864/60000]\n",
            "loss: 1.034013  [19264/60000]\n",
            "loss: 0.816175  [25664/60000]\n",
            "loss: 0.856520  [32064/60000]\n",
            "loss: 0.881770  [38464/60000]\n",
            "loss: 0.779812  [44864/60000]\n",
            "loss: 0.818281  [51264/60000]\n",
            "loss: 0.748260  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 76.0%, Avg loss: 0.737374 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.709584  [   64/60000]\n",
            "loss: 0.773141  [ 6464/60000]\n",
            "loss: 0.514966  [12864/60000]\n",
            "loss: 0.775091  [19264/60000]\n",
            "loss: 0.637151  [25664/60000]\n",
            "loss: 0.647146  [32064/60000]\n",
            "loss: 0.682952  [38464/60000]\n",
            "loss: 0.645298  [44864/60000]\n",
            "loss: 0.669508  [51264/60000]\n",
            "loss: 0.590100  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 79.1%, Avg loss: 0.603019 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.547428  [   64/60000]\n",
            "loss: 0.652636  [ 6464/60000]\n",
            "loss: 0.406571  [12864/60000]\n",
            "loss: 0.647868  [19264/60000]\n",
            "loss: 0.550659  [25664/60000]\n",
            "loss: 0.541080  [32064/60000]\n",
            "loss: 0.566679  [38464/60000]\n",
            "loss: 0.583656  [44864/60000]\n",
            "loss: 0.592984  [51264/60000]\n",
            "loss: 0.505151  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.6%, Avg loss: 0.524765 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.445233  [   64/60000]\n",
            "loss: 0.568359  [ 6464/60000]\n",
            "loss: 0.343302  [12864/60000]\n",
            "loss: 0.575112  [19264/60000]\n",
            "loss: 0.495261  [25664/60000]\n",
            "loss: 0.479169  [32064/60000]\n",
            "loss: 0.491967  [38464/60000]\n",
            "loss: 0.549407  [44864/60000]\n",
            "loss: 0.545776  [51264/60000]\n",
            "loss: 0.453296  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.0%, Avg loss: 0.475153 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.377062  [   64/60000]\n",
            "loss: 0.506837  [ 6464/60000]\n",
            "loss: 0.304608  [12864/60000]\n",
            "loss: 0.527888  [19264/60000]\n",
            "loss: 0.457821  [25664/60000]\n",
            "loss: 0.438211  [32064/60000]\n",
            "loss: 0.439809  [38464/60000]\n",
            "loss: 0.525573  [44864/60000]\n",
            "loss: 0.509793  [51264/60000]\n",
            "loss: 0.419244  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.3%, Avg loss: 0.439673 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.330451  [   64/60000]\n",
            "loss: 0.462530  [ 6464/60000]\n",
            "loss: 0.275056  [12864/60000]\n",
            "loss: 0.491780  [19264/60000]\n",
            "loss: 0.420374  [25664/60000]\n",
            "loss: 0.405883  [32064/60000]\n",
            "loss: 0.405296  [38464/60000]\n",
            "loss: 0.505088  [44864/60000]\n",
            "loss: 0.481680  [51264/60000]\n",
            "loss: 0.395274  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 85.1%, Avg loss: 0.412904 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.291833  [   64/60000]\n",
            "loss: 0.430412  [ 6464/60000]\n",
            "loss: 0.246681  [12864/60000]\n",
            "loss: 0.456241  [19264/60000]\n",
            "loss: 0.389816  [25664/60000]\n",
            "loss: 0.379934  [32064/60000]\n",
            "loss: 0.376795  [38464/60000]\n",
            "loss: 0.484123  [44864/60000]\n",
            "loss: 0.457445  [51264/60000]\n",
            "loss: 0.377414  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 85.7%, Avg loss: 0.393515 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.263674  [   64/60000]\n",
            "loss: 0.402961  [ 6464/60000]\n",
            "loss: 0.224696  [12864/60000]\n",
            "loss: 0.425296  [19264/60000]\n",
            "loss: 0.363419  [25664/60000]\n",
            "loss: 0.357114  [32064/60000]\n",
            "loss: 0.351596  [38464/60000]\n",
            "loss: 0.462593  [44864/60000]\n",
            "loss: 0.434262  [51264/60000]\n",
            "loss: 0.359400  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 86.3%, Avg loss: 0.377993 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.240602  [   64/60000]\n",
            "loss: 0.379006  [ 6464/60000]\n",
            "loss: 0.205060  [12864/60000]\n",
            "loss: 0.399361  [19264/60000]\n",
            "loss: 0.338281  [25664/60000]\n",
            "loss: 0.341384  [32064/60000]\n",
            "loss: 0.330380  [38464/60000]\n",
            "loss: 0.441706  [44864/60000]\n",
            "loss: 0.413125  [51264/60000]\n",
            "loss: 0.341326  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 86.7%, Avg loss: 0.364241 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    }
  ]
}